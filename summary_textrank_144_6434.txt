My name's Felix Hill and I'm going to be talking to you about deep learning and language understanding.So in the first section I'll talk a little bit about neural computation in general and language in general and then give some idea of why neural computation, deep learning and language might be an appropriate fit to come together and produce the sort of improvements and impressive language processing performance that we've seen over the last few years.In the second section I'll focus in on one particular neural language model, which I think is quite representative of a lot of the principles that govern all neural language models.And that model is the transformer which was released in 2018 and then in section three I'll go a bit deeper into a particular application of the transformer, that's the well known BERT model, and BERT in particular is an impressive demonstration of unsupervised learning and the ability of neural language models to transfer knowledge from one training environment to another.And then in the final section, we'll take a bit more of a look towards the future of language understanding and deep learning and to do that we'll delve into some work that's been done at DeepMind on grounded language learning, where we study the acquisition of language in deep neural networks that have the ability to interact and move around simulated environments.It's important to add that of course, natural language processing is an enormous field and there are many things that I'm not going to have the time to talk about during this lecture.So some of the most important ones are things like sequence to sequence models and specific applications of deep learning to neural machine translation.So you may have heard of models like GPT-2 or BERT, or WaveNet, which was developed in DeepMind.And all of these models have done really impressive things with respect to the various aspects of language processing that they focus on.So GPT-2 as a language model is now able to produce long streams of text which look like plausible stories and BERT has led to very large improvements on many language classification tasks.So if you think about all the sort of panorama of different things you might be able to apply language models or language processing technology to, like to a much greater sense than at any point in the past, neural computation and deep learning plays a role in those systems.So on the left we have systems which are almost now entirely based on neural networks from machine translation systems to speech synthesis systems and speech recognition and then on the right here, it's important to note that there are still many applications which do language processing but don't use deep learning or neural networks for all of their computation or even at all.So things like home assistants, which you might have to provide specific pieces of information from the internet, we're still a long way from building systems like that in an end-to-end fashion in deep neural networks.Having said that, the balance of this particular scale has moved a lot over the last few years and it's certainly a trend towards more applications of neural computation and neural networks in language processing applications.And it's obviously not just the number of publications but the effective quality of systems and models, which seems to be improving over this time.So, this is, sort of taken together, a bunch of evidence that, you know, deep learning has really been able to improve performance on a bunch of language processing applications and I think looking at that evidence, it raises the question of why deep learning, and models which have this neural computation at the heart of their processing, have been able to be so effective in language processing.What is it about deep learning and what is it about language which has sort of allowed this sort of effect to take place.So the first thing about language, it's often said that language is a process of manipulating symbols or that language processing involves symbolic data, operations on symbols.But of course, if those who think a little bit more about language specifically, have many reasons to believe that individual words that we might be passing to these models don't seem to behave like discrete symbols exactly.If we think about the word 'face' we can find it in many different contexts in language.And we call these differences word senses, but the important thing to note about the different senses of the word 'face' is that they're not entirely different.So this example shows and you will see these effects if you look at many other words that rather than discreet word sentences which are orthogonal to each other, we might be better off modeling this discrepancy in meaning within individual words as operations that can interact then, but are not necessarily the same.So we've seen then that words are not necessarily best modelled as discrete symbols.So this tells us that it can be things at one end of the sentence and things at the very other end of the sentence, which must be considered to interact in order for us to form the most satisfactory meaning when we read sequences of words.Whereas, our knowledge of peppers will tell us that they don't typically sneeze and therefore we don't think that the pepper sneeze is a very likely state of affairs and we look for other ways to make sense of the sentence and the correct way of making sense of that sentence, of the sentence in fact is more salient to us as we process it.So that's just a thought to bear in mind when we're thinking about optimal processes of language in deep learning models.So lots of people who consider and talk about language, particularly in the wider machine learning community, consider language to be compositional in the sense that the meaning can be computed simply by elegant operations on the individual parts.So even in something as simple as combining a colour adjective with a noun, there's all sorts of factors at play telling us exactly how those meanings combine that don't seem to be equivalent from one pair of words to the next.This doesn't always happen when we combine words, but it does sometimes.So these are kind of wacky effects of how meanings interact when two words come together and it's not necessarily easy to explain them in a model which treated every pair of words fed into that model with exactly the same function to combine their meanings, it very much seems to me that what's instead happening is that whatever function is combining the meanings is taking into account the individual meanings of the components going into that function and in, in additional, additionally, that function may well need to take into account a wider knowledge of typical things we might encounter in the world and how their properties might fit together under the constraints of the world as we know it.So just to summarise, we've seen in this section that words have many related senses and that they're not necessarily characterised as sort of perfectly idealised discrete symbols.And finally, when we're thinking about building models of how word meanings might combine, we've seen that functions that combine meanings will probably need to take into account what the inputs to those functions are in order to come up with the best bespoke way of combining for those particular words.And we've even suggested that they may well also need a widened sense of how the world works and how things can naturally fit together in order to eventually arrive at the optimal representation for the combination of meanings in each particular case.So, in the first part we talked about particular aspects of language and particular aspects of neural computation that have that seem to fit together in a particularly appropriate way, such that, they define certain ways in which a computational model might need to behave in order to capture the ways that meaning works in language.So in this section we're going to talk much more concretely about a specific model, which was published just a couple of years ago and has had an incredible impact on a large number of natural language processing problems from machine translation to sentence classification and essentially any problem that requires a model to process a sentence or a passage of multiple sentences and compute some sort of behavioural prediction based on that.And in this section I'll talk about the details of the transformer and just refer back to those aspects of language processing that we saw in the first section in order to give some intuition about why the transformer might be so effective when it processes language.So the transformer contains a distributed representation of words in its first layer which is something it has in common with almost any neural language models now.And what do I mean by a distributed representation of words?But in general with language processing applications, because we have texts stored in digital form, we don't need to go through that phase and subject our model to having to learn to process pixels.And in most applications of neural language models these days, that can either be character level, which is where we pass each unit as an individual letter or it can be word level, which is where we split the input according to white space in the text and then we pass each of the individual words to the model as discrete different symbols.But of course, as we've talked about in the last section, a model which just takes symbols and treats them as symbols might not be optimal for capturing all of the aspects of meaning that we see in natural language.So instead of doing that, the developers of neural language models have come up with a procedure which allows the model to be more flexible than which would be represents, in the ways of which it represents words.So let's say we do take the decision to chop up our input text according to individual words.What we typically do in a neural language model then is pass each of the words to an input layer and that input layer contains a particular unit for each, corresponding to each of the words in the vocabulary of the model.Now that dimension we can think of as the word representation dimension or the word embedding dimension and when the model sees a given word, we turn on the unit corresponding to that word and we leave all of the other units at zero.So we put an activation of one on unit corresponding to the word leave all of the other weights as zero and we've marked those weights in this diagram here with yellow and light blue shows the space occupied by the whole layer of input weights for the model.So we might find that representing words in a space like that allows words to move together in the space if it's useful for the model to represent them as somewhat similar and to move further away in that space if it's useful for the model to represent them as different.So this gives the model the flexibility to move its representation of individual words around as it sees fit and the best way to achieve its objective.So just to recap, this is the first layer of many neural language models, including the transformer.So if we have a total of capital V words in our vocabulary and if capital D is the dimension of the vector that we're going to represent each of these words with in a floating point vector, then the total number of weights that we have in the first layer is V multiplied by D and we end up with a D dimensional Euclidean space with which to represent these input units in the model.Now this idea of representing words or letters or whatever we take as the input units to a model in some sort of high dimensional floating value, real valued vector space is actually quite an old idea.If we go back to 1991 Mikkulainen and Dyer produced a language, a neural language model with much less computational power than current models have, but it still tried to execute this principle of representing input words in this distributed geometric space.And it was able to exhibit certain types of interesting generalisation when trained on real texts that a model which represented words as individual discrete symbols wouldn't be able to represent or achieve.And in this paper Elman analysed the distributed representations corresponding to lots of different words as he trained the model on sequences of, sort of, subject verb object style sequences of natural language style snippets.And the objective of this model was just to represent a sequence of words such that the model was able to optimally predict the next word with as much accuracy as possible.And what Elman found when he analysed the way that the model was distri-, was representing these words internally was that of all the words in his vocabulary, they started to cluster together in this geometric space such as the words with similar meanings came together.And this tells us that neural language models, as they experience more and more text, start to slowly infer the underlying structures in language which we might be able to perceive as language users such as subject, object, verb and how things fit together like that as well as an emergent categorical semantic structure where we see that certain classes of different types of words naturally fit together.Distributed representations of words have been a part of neural language models as I pointed out since the early nineties.So this then gives us a set of weights correspond-, it's a probability distribution, which gives us a set of weights between zero and one, so for a given word 'beetle', we get a set of weights, one for each of the words in the input telling us to what extent is there a strong interaction between the word 'beetle' and that other word.So what we end up with then for each word like 'beetle' is that we take a small amount of the value of each of the other words plus some of the value of the word 'beetle' through to the next layer of the transformer.So notice that having performed this transformer layer, we haven't reduced the number of embeddings in the model in any way, we still have a representation corresponding to the word 'beetle' that we started with, but that representation has been updated or modulated, conditioned exactly on information about how well it corresponds or how well it should interact with all of the other words in the input.But it does give the model many independent ways with which it can represent interactions between the words and the inputs.Well, in the examples I gave in the previous section about language, one thing that should have maybe come across is the importance of, or the role of our expectations in forming a consistent representation of what a particular input is.And so these sorts of top down influences our expectations influencing how we actually combine the inputs in language are really common in many different contexts.And if you think about skip connections, it's not a perfect model of this, but it does give the transformer a rudimentary ability to allow its representations of things at a higher level of processing to interact with this representations of things at a lower level of processing.At that point after computing many different interactions, the model might form a consistent sense of the fact that a meaning needs to be understood in a particular way, but of course those top down influences tell us that that expectation of what the meaning might be should actually feed back and allow us to remodulate how we understand the input.Now, if you were aware, if you were paying attention during the explanation, you may well have noticed that none of the operations that I described on the input words took into account the actual relative order of the words in the input.So there was no way that a model like this would have any ability to express the fact that certain words appear close together in the input or certain words appear further apart.And of course we know in language that the word order can tell us some important things about what the overall sentence means.So positional encoding is just a way of determining a set of scalar constants which are added to the word embedding vector after say let's say in the lowest level of the transformer it can be added before the first self-attention layer, but just after the word embedding layer, and those scalars combined with the word embedding to mean that whatever, if a word appears in a particular slot in the input, regardless of the fact that it's embedding weights will necessarily be the same, the actual effective representation that the transformer sees will be slightly different depending on where it appears in the input.So to achieve this sort of thing, you just need, the model just needs a set of small scalars which are different in each of the possible locations that are word could appear in the input.And they use a nice sinusoidal function which has various properties which may well be more desirable than just being, allowing the word to discriminate words according to their position.Because in fact that sinusoidal function gives the model a slight prior to pay attention to relationships of a certain wavelength, a certain distance across the input and each particular unit in the embedding representation can then specialise at recognising interactions or correspondences at a different distance from a given word.So, unlike if you think about models like a recurrent neural network or an LSTM, those models have the notion of order built in because they process input sequentially, one word after the other according to a process transitioning the state from the, from its position after reading one word to after reading two words, to after reading three words, to after reading four words.The model has sort of natively, in its native functional form, it has no awareness of word order and we have to add on these additional positional encodings to give the model a weak awareness of word order.But the transformer actually performs better than RNNs and LSTMs on a lot of language tasks and this maybe tells us that it's easier to learn the word, the notion of word order for the few cases or for the number of cases where it's actually important in language than it is to be given the notion of word order automatically, but to have to learn the very difficult process of paying attention to things a long time in the past.With a transformer that path that the gradient has to go through is much shorter because there's no prior favouring of things which are close together instead of the gradient path that the model needs to go through to connect any two words in the input is equivalent and in fact it indeed is shorter on average than it is in recurrent neural networks.So just to summarise this section: we saw in the previous section that words shouldn't necessarily be thought of as independent discrete symbols and that disambiguating their meaning can depend a lot on the context but not only on the immediate context which is closest to those words, but on potentially distant context of the information encoded in words a long way away.We've also seen that functions which models use in order to combine the meaning of two words should take into account the meaning of those words and if possible, take into account a wider general knowledge of how things typically combine in order to allow that to modulate the interactions between the words coming in and we think about the architectural components I talked about in the transformer.The multi-head processing is one way of getting at this notion that words are not discrete symbols because it naturally gives the transformer even in one feed, single feed forward pass the opportunity to represent each word at each layer with n, let's say four, different possible contextualised representations and of course going back longer term just the general notion of representing words as distributed representations and allowing words with similar meanings to occupy local areas in a large geometric vector space also allows the model to express this non-discreet nature of word meaning in a very eloquent way.So interactions over words that are next to each other are not particularly favoured over minor interactions.Another fact is that the more layers we have, the more chance the model has to learn as it moderates this representation of different things, how interactions might take place at different levels of abstraction as the model goes, continues to reprocess the model, the input.The multiplication of a matrix by a vector is precisely the operation of a function which combines word meanings according to the meanings of the words themselves and those operations are common in most, many neural language models, but are a really important part of the transformer architecture.So hopefully this section has given you some intuition about how a transformer works, but also some intuition about maybe why it works, why it is that the various components in the transformer improve on a model's ability to process language because of the way that we think meaning works in a very sort of intricate and interactive way when we understand linguistic input.In the last section, we introduced the transformer and we talked about how various components within the transformer combine to make it a very powerful process, a very powerful model for processing sentences and combinations or sequences of words.It's a way of training transformer models in order to allow them to excel at a wide range of different language tasks.But before I do that, I also just want to go back to our points about the nature of language and discuss one more issue which I think is quite motivating when we think about how transformers are applied in the model that I'm going to talk about in this section.But of course when we start to process and make sense of these sentences, it feels very clear to us as native English speakers, that there's quite a difference in the way that the words in those sentences have to relate to each other in order for us to sort of construct the meanings in our head.And then of course there's again, other than that background knowledge of how the world works, how fruit flies are, there's also this kind of more linguistic knowledge of sentences we may well have already previously understood in which the meaning seems to combine in a similar way to 'fruit flies like a banana'.And a lot of the places where such a model needs to get its understanding of the world and its understanding of language and those considerations lead us to add a fifth point to these many characteristics of language, which is that when we actually form an understanding, you know, it really does seem to be a process of balancing our existing knowledge and that could be knowledge of language and also knowledge of the world with the input with the particular facts of the thing that's currently coming into the model.But the key insight with BERT is that rather than training a transformer just to understand the inputs to the sentences which the model is currently considering, a process of pre-training takes place in which the weights within the model are endowed with knowledge of a much wider range of text in this case, which can plausibly give the model that background knowledge which is really necessary for forming a coherent understanding of the total of the different types of sentences a language understanding processor needs to, to be able to understand.So the important thing to remember when considering how BERT works is that a transformer as described in the previous section really is just a mapping from a set of distributed word representations to another set of distributed word representations.And corresponding to each of the words in the input, the exact same set of words that were the input.So given that fact that a transformer is just a mapping from a set of word representations to a modified set of word representations of the same length, there's quite aneasy way in which we could train such a model in order to extract knowledge from an enormous amount of texts that we might just have lying around.So in particular, the insight from BERT is precisely how can we get knowledge into the weights of such a model without requiring problems or data which has been labeled by human experts or other, some other mechanism in order to give the model sort of knowledge of what's the right classification or what's the right answer to make.So how can we get knowledge into a model, a transformer model in an unsupervised way?So the way this works is the following, the authors just considered the problem of mapping a particular sequence of words to the exact same sequence of words.And instead of having to predict all of the words in the input sentence, the model just has to make a prediction conditioned on the output embedding for the missing word of what that missing word was.So it just has to answer the question, you know, here's a sentence with a missing word in it, 'sucking up '...' from words', and the model just has to make the prediction that the missing word in that case is knowledge.And when training the model, the authors of BERT do that with 15% of words at random.So they ran, they present sentences from any, any particular place where we might be able to get running text language and the authors mask out words with a probability of 15% and then ask the model to make a prediction and backpropagate the cost, which is essentially, the likelihood, the negative log likelihood of the model, having predicted that word over all of the other words in its vocabulary.In this case, the model really does just need to retain which word is in a particular point in the sentence and at the output representation corresponding to that point, conditioned on that, make a prediction of what that word was.But in order for BERT to be an effective language processor, the authors wanted it to also be able to, to be aware of the flow of how meaning works on a longer scale than just within a particular sentence.So in order to achieve this, they came up with an additional mechanism for training the weights in the BERT model, which is complimentary to the masked language model objective.Then as input to the model, the model is presented with not one sentence, but two sentences in this case and so there's the additional input token, then there's the first sentence, then a separation token, and then the second sentence, and that's all passed to the transformer and it's processed through in parallel.So like combining next sentence prediction and masked language modeling, slowly the weights of this large transformer, the BERT transformer, gradually start to acquire knowledge of how words interact in sentences typically maybe abstract knowledge of the typical ways in which meaning flows through sentences.And of course, the spaces in which they represent each of the individual words at various levels of the stack, things start to happen like words that have similar meanings start to come close together.The model might require to separate them out into the different parallel heads if words have various different senses.And so, a lot of the general knowledge that we talked about being very necessary for forming a consistent and coherent representation of loads of different language sentences can start to be introduced into the weights of this model as it trains according to these unsupervised objectives.And in this case, the way that BERT is then evaluated is by taking its knowledge in all of those ways and using that as a start process to train on many specific language understanding tasks.So, in order to apply BERT to these models, the BERT weights, which are trained on all of the unsupervised objectives, are then taken and the data specific to each of these tasks is passed through the BERT model and then BERT is, the BERT weights are updated according to the signal from the supervised learning signal from these actual specific language understanding tasks.And it's also necessary in many cases when fine tuning in this way to add in a little bit of machinery onto the top of BERT because, you know, in the standard BERT architecture, it's just making predictions where it outputs a number of distributed representations at the top of this transformer model.So just doing this massively improves the performance of any models which aim to exhibit some sort of general understanding of language.What I mean by that is, any model which is intending to be trained on a wide range of different tasks, using the BERT style approach, so transferring knowledge from an enormous running text corpora, via fine tuning to those specific tasks, has led to a really strong and significant performance on a large number of these tasks.Now a few years before BERT, a model called ELMO and a couple of other models, started to show that there was some promise in sharing more than just those word embedding weights, but actually sharing a large amount of functions which learned to combine weights when pretrained on some task agnostic objective and transferred to specific tasks.So we've now acquired five interesting principles of how language and meaning seem to interact when we understand the sentence.And when we've added this fifth one: understanding is balancing input with knowledge that we've had already or our general knowledge of the world.And we've talked about BERT as a mechanism for endowing models with something like a general knowledge that may be necessary.And we've shown that, in fact, indeed it is very important on a lot of language understanding tasks to have this sort of prior knowledge acquired from a massive range of different experiences and different types of texts.So in the next section we'll look a bit forward to other sources of information which may plausibly be useful for different language understanding models because of course, BERT only has the means to acquire knowledge through text whereas if you think about the fruit fly example or time flying like an arrow, those sorts of examples tell us that there are many other sources of information that we may have used in order to gain the general conceptual or world knowledge required to actually make sense of language.So in the last section we saw how the BERT model is a really exciting example of transferring knowledge from an enormous amount of text, to apply that knowledge to very specific language tasks that maybe have a small amount of data from which to learn.And this works in part because of the critical importance of general knowledge in understanding language and we need ways in which models can acquire general principles of how language works and how word meanings fit together in order to make high quality predictions for a range of different language tasks.Now in this section we're going to talk about further ways in which we might be able to endow models with general or conceptual knowledge which they can then apply to language related tasks.And in particular in a way that's not accessible to the BERT model, which is the ability to extract knowledge, general knowledge and conceptual knowledge from our surroundings, which is something as humans that we are doing all the time.So as well as the objective of masked language modeling and next sentence prediction that we saw with BERT, there's also exciting techniques in the field of computer vision, that often involve things like missing parts of an image and making predictions about whether or not that part of the image is the correct part or of which pixels would most appropriately fit in to that part of an image or maybe contrasting incorrect parts of images with correct parts of images and things like that.And, of course, in the world of learning when it comes to jointly learning language and behaviour, which involves often reinforcement learning on those sorts of tasks are techniques for having agents develop a more robust understanding of their surroundings and possibly import what's known as a model of their world.Whether or not this knowledge would be knowledge which could serve the agent's ability to understand or use language.And the way we did that was as well as creating loads of random rooms with different objects positioned in different places in this simulation, we also created a bunch of questions such that for any random room that was created the agent could find in the environment questions which could plausibly be answered, so examples of the sorts of questions we asked were things like, 'what is the colour of the table?And importantly, being able to answer these questions requires a particular type of knowledge, that's propositional knowledge, the knowledge, the ability to tell whether something's true or false in our environment and that's often contrasted especially by philosophers with procedural knowledge, which is just the sort of instinctive knowledge that maybe a reinforcement learning agent would naturally have when it learns to solve control problems in a very fast and precise way.So the agent just has to, the learning algorithm just has to find a way based on that experience to aggregate general knowledge into the agent such that when the question the QA decoder is queued with the state of the agent at the end of the episode and queued with the question, it's possible to combine those two pieces of knowledge and answer with 'dinosaur'.So to do this effectively, the agent needs a large amount of general knowledge about how things are arranged in the environment around it such that the QA decoder can take that knowledge and make predictions about the answers to questions.A little bit like the sorts of masked language model prediction that BERT's making, but where the agent has to given a certain time point in the episode, a predictive loss, a predictive engine an overshoot engine, takes the current memory state of the agent at that time point and rolls forward in time.So that's just a small insight into work that's going on in DeepMind where we're starting to consider how we can aggregate knowledge from the general environment as well as knowledge from large amounts of text into a single model which can start to combine this sort of conceptual understanding and general knowledge understanding and our understanding and a really strong understanding of language into a single agent which can come up with a coherent and strong ability to form the meaning of statements and sentences and also to take that knowledge to answer questions, to produce language and to enact policies, enabling it to do things things in complex environments.So we've talked about various aspects of language which make neural networks and deep networks particularly appropriate models for capturing the way that meaning works.So in particular, we raised the fact that words are not discrete symbols but they actually almost always have some sense of different related senses that disambiguation is a huge part of understanding language and then that can critically depend very often on context.And so if we look at these features or these aspects of language, the mechanisms that I've discussed today cover them reasonably well and hopefully they shed some light on why neural networks and interactive processing architectures that obey the sort of disparate principles of neural computation.And distributed representations are particularly effective for language processing.But of course, it should be said that there are many aspects of language processing that the work I've talked about just doesn't start to approximate, doesn't start to capture.So our models are not currently able to do things like understanding the intentions of others or reflecting on how language is used to communicate and do things.And, you know, we need to make a lot more progress in these areas if we're actually going to arrive at agents which are truly able to understand language.So yeah, just as a final note, I think it's interesting that before deep learning really exhibited its success on language processing problems, a typical view of language understanding was what I call the pipeline view, which was that each independent, each part of processing language from the letters to the words, to the syntax and to the meaning and then eventually to some prediction could be thought of relatively independently as a separate process.But now that we've reflected on how language works and in particular taking in all of the evidence from the effectiveness of different neural language models on language processing tasks, I think maybe this is a more effective or more realistic schematic of how language processing should be thought of.Those two things input to our system but critically, it's that input combined with our general background knowledge of the world of knowledge of language, which together allow us to arrive at some sort of plausible meaning for everything that we hear or everything that we might say.So anyway, I hope you've enjoyed this lecture and it's given you some insight into why language and language understanding is such an interesting problem for computational models to try and tackle.