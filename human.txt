My name's Felix Hill and I'm going to be talking to you about deep learning and language understanding. So in the first section I'll talk a little bit about neural computation in general and language in general and then give some idea of why neural computation, deep learning and language might be an appropriate fit to come together and produce the sort of improvements and impressive language processing performance that we've seen over the last few years. In the second section I'll focus in on one particular neural language model, which I think is quite representative of a lot of the principles that govern all neural language models. And that model is the transformer which was released in 2018 and then in section three I'll go a bit deeper into a particular application of the transformer, that's the well known BERT model, and BERT in particular is an impressive demonstration of unsupervised learning and the ability of neural language models to transfer knowledge from one training environment to another. And then in the final section, we'll take a bit more of a look towards the future of language understanding and deep learning and to do that we'll delve into some work that's been done at DeepMind on grounded language learning, where we study the acquisition of language in deep neural networks that have the ability to interact and move around simulated environments. It's important to add that of course, natural language processing is an enormous field and there are many things that I'm not going to have the time to talk about during this lecture. So GPT-2 as a language model is now able to produce long streams of text which look like plausible stories and BERT has led to very large improvements on many language classification tasks. And of course WaveNet has led to fantastic performance in speech synthesis where we're now are able to synthesize voices for various speech applications with much more fidelity than was previously possible. So if you think about all the sort of panorama of different things you might be able to apply language models or language processing technology to, like to a much greater sense than at any point in the past, neural computation and deep learning plays a role in those systems. So on the left we have systems which are almost now entirely based on neural networks from machine translation systems to speech synthesis systems and speech recognition and then on the right here, it's important to note that there are still many applications which do language processing but don't use deep learning or neural networks for all of their computation or even at all. So the GLUE benchmarks are just a sort of intended to be a representative of language classification challenges, things like reading a couple of statements saying whether one of them entails another one or maybe classifying them as positive sentiment or negative sentiment, things like that. So, this is, sort of taken together, a bunch of evidence that, you know, deep learning has really been able to improve performance on a bunch of language processing applications and I think looking at that evidence, it raises the question of why deep learning, and models which have this neural computation at the heart of their processing, have been able to be so effective in language processing. So the first thing about language, it's often said that language is a process of manipulating symbols or that language processing involves symbolic data, operations on symbols. But of course, if those who think a little bit more about language specifically, have many reasons to believe that individual words that we might be passing to these models don't seem to behave like discrete symbols exactly. And we call these differences word senses, but the important thing to note about the different senses of the word 'face' is that they're not entirely different. So if we think of all of those as sort of features or properties of this sense of 'face' and then when we think about the sense 'the clock face', we see that it actually shares some of those properties but not all of them. So this example shows and you will see these effects if you look at many other words that rather than discreet word sentences which are orthogonal to each other, we might be better off modeling this discrepancy in meaning within individual words as operations that can interact then, but are not necessarily the same. So this sort of example intuitively gives us some justification for thinking, well, maybe it's the interactions between the individual tokens that we're looking at and all of the things around them, which actually allow us to solve the mystery of which possible sort of sense of a word we might be looking at at any one time. And in particular, what we probably do according to this example at least, is think about the whole sentence and think what's the most likely interpretation of the whole sentence and that in itself informs the individual interpretation of the particular characters where ambiguity might be. And we've also seen that in order to disambiguate meanings that naturally fit between these word-like things, we better off be considering wider context in order to modulate those computations. Another very important fact about language is that the important interactions which we may well need to model, can very often be non-local. So this tells us that it can be things at one end of the sentence and things at the very other end of the sentence, which must be considered to interact in order for us to form the most satisfactory meaning when we read sequences of words. So in particular it seems to be that the, the three word phrase, 'the dog barked' seems to capture our attention and we sort of have an urge to consider that it's actually the dog barking, in a way that's more strong than in the other case where we don't have a such a strong urge to consider, it's the pepper that sneezed. Well, these sorts of examples seem to tell us that those urges can come from our underlying understanding of the world, our understanding of the meaning of dog and barking and the fact that those are very likely to come together and form and describe a particular situation. Whereas, our knowledge of peppers will tell us that they don't typically sneeze and therefore we don't think that the pepper sneeze is a very likely state of affairs and we look for other ways to make sense of the sentence and the correct way of making sense of that sentence, of the sentence in fact is more salient to us as we process it. So lots of people who consider and talk about language, particularly in the wider machine learning community, consider language to be compositional in the sense that the meaning can be computed simply by elegant operations on the individual parts. It should be a function which really takes into account the individual meanings in a particular scenario before deciding the best way to combine those meanings. So even in something as simple as combining a colour adjective with a noun, there's all sorts of factors at play telling us exactly how those meanings combine that don't seem to be equivalent from one pair of words to the next. So these are kind of wacky effects of how meanings interact when two words come together and it's not necessarily easy to explain them in a model which treated every pair of words fed into that model with exactly the same function to combine their meanings, it very much seems to me that what's instead happening is that whatever function is combining the meanings is taking into account the individual meanings of the components going into that function and in, in additional, additionally, that function may well need to take into account a wider knowledge of typical things we might encounter in the world and how their properties might fit together under the constraints of the world as we know it. So in this section we're going to talk much more concretely about a specific model, which was published just a couple of years ago and has had an incredible impact on a large number of natural language processing problems from machine translation to sentence classification and essentially any problem that requires a model to process a sentence or a passage of multiple sentences and compute some sort of behavioural prediction based on that. So it's fair to say that for any of any problem of that form transformer is probably the state of the art method or some variant of a transformer is the best way to for the model to learn and to learn to extract the signal from those sentences in order to make optimal protections. So the transformer contains a distributed representation of words in its first layer which is something it has in common with almost any neural language models now. Well, the first thing that we do when we construct a neural language model is we have to determine what is the vocabulary on which the model is going to operate. But of course, as we've talked about in the last section, a model which just takes symbols and treats them as symbols might not be optimal for capturing all of the aspects of meaning that we see in natural language. So instead of doing that, the developers of neural language models have come up with a procedure which allows the model to be more flexible than which would be represents, in the ways of which it represents words. So to get such a list, we might scan hundreds of thousands of pages of texts and count all the works that we find there, and then we can take some subset of the words which appear the most frequently or alternatively if we have lots of memory and a really big model, we can take all of the words and allow all of those to be in the vocabulary of the model. What we typically do in a neural language model then is pass each of the words to an input layer and that input layer contains a particular unit for each, corresponding to each of the words in the vocabulary of the model. But importantly, those units are then connected to a set of weights and it's always, each unit is connected to the same number of weights and those weights connect to a set of units of a particular dimension. Now that dimension we can think of as the word representation dimension or the word embedding dimension and when the model sees a given word, we turn on the unit corresponding to that word and we leave all of the other units at zero. So we put an activation of one on unit corresponding to the word leave all of the other weights as zero and we've marked those weights in this diagram here with yellow and light blue shows the space occupied by the whole layer of input weights for the model. So we might find that representing words in a space like that allows words to move together in the space if it's useful for the model to represent them as somewhat similar and to move further away in that space if it's useful for the model to represent them as different. So this gives the model the flexibility to move its representation of individual words around as it sees fit and the best way to achieve its objective. So if we have a total of capital V words in our vocabulary and if capital D is the dimension of the vector that we're going to represent each of these words with in a floating point vector, then the total number of weights that we have in the first layer is V multiplied by D and we end up with a D dimensional Euclidean space with which to represent these input units in the model. Now this idea of representing words or letters or whatever we take as the input units to a model in some sort of high dimensional floating value, real valued vector space is actually quite an old idea. And what Elman found when he analysed the way that the model was distri-, was representing these words internally was that of all the words in his vocabulary, they started to cluster together in this geometric space such as the words with similar meanings came together. And importantly also words with similar syntactic roles, so things like verbs or nouns or subjects or objects also started to cluster together in the space. And this tells us that neural language models, as they experience more and more text, start to slowly infer the underlying structures in language which we might be able to perceive as language users such as subject, object, verb and how things fit together like that as well as an emergent categorical semantic structure where we see that certain classes of different types of words naturally fit together. So the next stage, the transformer, computes what's called a self-attention operation. Well for any self-attention operation, there are three matrices containing the weights which parameterise the operation. And each of these matrices have independent weights in the transformer and we can, their dimensions are such that they can naturally multiply, in this case, I've written it as post multiplication of the distributed word vector that I talked about in the first section. And importantly as the self-attention operation is carried out, these weights are applied equally and in exactly the same way to each of the words in the input. So we end up with for every individual word vector I've written here 'e beetle' corresponding to the word 'beetle' in the input, we ended up with three further vectors corresponding to multiplying that vector by the matrix WQ, the matrix WK and the matrix WV. So those three additional vectors we can call bold Q, bold K and bold V and we can call those, they are typically called the Query Vector, the Key Vector and the Value Vector for this self-attention operation corresponding to each of the words. So in particular with the query vector, we produce an operation where for every word we take the query vector corresponding to that word and we compute the inner product, the dot product of that word, with the value, with the, with the key vector corresponding to each of the other words. And by taking a dot product in that way, we get a scalar, and then we can, we want to understand how big is that scalar relative to an average scalar that we would get if we just took that operation arbitrarily. So essentially we want to give the model the power to represent how strong should the connection between these two words be. And, in order for that to be a nice normalised distribution over all the possible strengths computed by the model, we first work out the inner product of the query value with the key value of a particular word. And then we divide that number by the dot product of the, well, we need to normalise by quantity corresponding to the dot product of that query vector with each of the key vectors of the other words. And the way we do that is, we compute those values and we pass all of those values and the dot products through a softmax layer, which gives us a distribution, so it normalises, it exponentiates and normalises, such that we get a nice smooth distribution corresponding to how well each of the queries corresponds to each of the keys of the words in the input. So this then gives us a set of weights correspond-, it's a probability distribution, which gives us a set of weights between zero and one, so for a given word 'beetle', we get a set of weights, one for each of the words in the input telling us to what extent is there a strong interaction between the word 'beetle' and that other word. So in this case, the way I've marked it in the slide is that the strongest interaction when we do this operation is with the word 'drove' and that might be because the word 'drove' tells us in particular that this beetle is not the animal type of beetle, but it should in fact be thought of as the car beetle. So notice that having performed this transformer layer, we haven't reduced the number of embeddings in the model in any way, we still have a representation corresponding to the word 'beetle' that we started with, but that representation has been updated or modulated, conditioned exactly on information about how well it corresponds or how well it should interact with all of the other words in the input. And that computation can be computed in parallel, which makes the transformer quite fast to feed forward in today's deep learning libraries. And so for one application of a self-attention layer we end up with the same number of distributed representations coming out as we had going in. And within the mechanism, the only weights that we learned are those single matrix giving us the queries, a second matrix, giving us the keys and a third matrix, which gives us the value representations of course those matrices are then applied to each of the individual words. But it's not just this self-attention layer that gives the transformer it's expressability and power is actually an operation known as a multi-head self-attention, which basically takes the operation I just talked about and reapplies it four different times in parallel. So if you imagine the operation that I just spoke about being parameterised by three matrices, WQ, WK and WV, well we can repeat that process with three additional independent matrices. So we end up with four independent and parallelisable self-attention operations, each computed on the input words of a particular layer in order to get us through to the next layer. In practice, then, what the developers of the transformer recommend is that each of our self-attention layers actually effectively reduces the dimensionality of the input vectors. What that would do is mean that the output of WV, which is the value vector, which gets passed to the next layer of the transformer, that can be arbitrarily small. And so each self-attention layer independently takes 100 dimensional vector and returns 25 dimensional vector for each unit, for each word in our inputs. But if we do that four times, then we end up with four 25 dimensional vectors and those can be aggregated, in fact, in the transformer they're passed through an additional linear layer which is parameterised by matrix W0 but then concatenated to return overall a vector of the same dimension, 100 units as was the dimensionality of the input. And that makes it a relatively practical tool, which doesn't lead to an enormous explosion in the memory requirements of the models. Now after that multi-head self-attention layer, the model does what's called a feed forward layer. So conceptually this is less interesting, but essentially the representations that the output of the multi-head self-attention layer are then multiplied again by a linear layer, there's a rectified linear unit non-linearity and then they're actually expanded out and dimensioned somewhat and then reduced again in dimension with another linear layer. So whenever we apply a multi-head self-attention layer or indeed a linear layer, the transformer also gives the model the option to ignore that computation and instead to pass the activations that were at the input to that multi-head self-attention layer direct, to bypass the self-attention layer and go through to the point of the network of which the output is coming out of that self-attention layer. And then that is added to the output of the self-attention layer, passed through a layer normalisation layer and then that represents the actual output of the whole unit that, the whole part of the network, the whole module which is doing the multi-head self-attention. And if you think about skip connections, it's not a perfect model of this, but it does give the transformer a rudimentary ability to allow its representations of things at a higher level of processing to interact with this representations of things at a lower level of processing. So positional encoding is just a way of determining a set of scalar constants which are added to the word embedding vector after say let's say in the lowest level of the transformer it can be added before the first self-attention layer, but just after the word embedding layer, and those scalars combined with the word embedding to mean that whatever, if a word appears in a particular slot in the input, regardless of the fact that it's embedding weights will necessarily be the same, the actual effective representation that the transformer sees will be slightly different depending on where it appears in the input. Because in fact that sinusoidal function gives the model a slight prior to pay attention to relationships of a certain wavelength, a certain distance across the input and each particular unit in the embedding representation can then specialise at recognising interactions or correspondences at a different distance from a given word. So, unlike if you think about models like a recurrent neural network or an LSTM, those models have the notion of order built in because they process input sequentially, one word after the other according to a process transitioning the state from the, from its position after reading one word to after reading two words, to after reading three words, to after reading four words. But then it, it's harder for that model to remember to pay attention to things a long time in the past even if those things actually end up having a really important influence on what I'm currently looking at now. With a transformer that path that the gradient has to go through is much shorter because there's no prior favouring of things which are close together instead of the gradient path that the model needs to go through to connect any two words in the input is equivalent and in fact it indeed is shorter on average than it is in recurrent neural networks. The multi-head processing is one way of getting at this notion that words are not discrete symbols because it naturally gives the transformer even in one feed, single feed forward pass the opportunity to represent each word at each layer with n, let's say four, different possible contextualised representations and of course going back longer term just the general notion of representing words as distributed representations and allowing words with similar meanings to occupy local areas in a large geometric vector space also allows the model to express this non-discreet nature of word meaning in a very eloquent way. Now the fact that distribution depends on context is very nicely modeled by self-attention precisely gives the meaning of every word to be critically dependent on the meaning of all the other words in a given input stream. And the fact that that context could be non-local as I've just talked about is very nicely modeled by the self-attention mechanism because the gradient flow from the particular point I'm in at a sentence to any other point in the sentence, is the same. Another fact is that the more layers we have, the more chance the model has to learn as it moderates this representation of different things, how interactions might take place at different levels of abstraction as the model goes, continues to reprocess the model, the input. And finally on this point about how meaning combined and the fact that the meaning, the ways in which the meanings of two words combine seems to often depend on the particular meaning of those words and also top down effects. the multiplication of a matrix by a vector is precisely the operation of a function which combines word meanings according to the meanings of the words themselves and those operations are common in most, many neural language models, but are a really important part of the transformer architecture. So if we're going to come up with a general language understanding engine that's able to cope with all these different types of processes and constraints which are involved in understanding a sentence, then there's obviously a lot of places where such a model needs to get its experience. And a lot of the places where such a model needs to get its understanding of the world and its understanding of language and those considerations lead us to add a fifth point to these many characteristics of language, which is that when we actually form an understanding, you know, it really does seem to be a process of balancing our existing knowledge and that could be knowledge of language and also knowledge of the world with the input with the particular facts of the thing that's currently coming into the model. But the key insight with BERT is that rather than training a transformer just to understand the inputs to the sentences which the model is currently considering, a process of pre-training takes place in which the weights within the model are endowed with knowledge of a much wider range of text in this case, which can plausibly give the model that background knowledge which is really necessary for forming a coherent understanding of the total of the different types of sentences a language understanding processor needs to, to be able to understand. So the important thing to remember when considering how BERT works is that a transformer as described in the previous section really is just a mapping from a set of distributed word representations to another set of distributed word representations. So if I pass a sentence to transform a model, it'll very quickly compute a set of embeddings for those, for each of the words in that sentence. And then it will output, it will pass them through self-attention models and output a set of embeddings for each of the words in that sentence. So given that fact that a transformer is just a mapping from a set of word representations to a modified set of word representations of the same length, there's quite aneasy way in which we could train such a model in order to extract knowledge from an enormous amount of texts that we might just have lying around. So in particular, the insight from BERT is precisely how can we get knowledge into the weights of such a model without requiring problems or data which has been labeled by human experts or other, some other mechanism in order to give the model sort of knowledge of what's the right classification or what's the right answer to make. And the approach that the authors of BERT take is firstly, by means of a masked language model pretraining phase. So the way this works is the following, the authors just considered the problem of mapping a particular sequence of words to the exact same sequence of words. So the job of this transformer in theory is just to represent a sentence, for example, and then output a sentence at the very top of its network. But rather than, rather than having the model output the exact same sentence, instead in the input to the model, one of the words is masked out. And instead of having to predict all of the words in the input sentence, the model just has to make a prediction conditioned on the output embedding for the missing word of what that missing word was. So they ran, they present sentences from any, any particular place where we might be able to get running text language and the authors mask out words with a probability of 15% and then ask the model to make a prediction and backpropagate the cost, which is essentially, the likelihood, the negative log likelihood of the model, having predicted that word over all of the other words in its vocabulary. But one thing that the authors noticed is that, if they trained the model in that way, then on the test set when they came to use this model, of course, in the input there wouldn't actually be any tokens masked out. So for a small amount of the time, instead of masking out a word, they have the model make a prediction of which word is missing even though they didn't actually mask a word out. In this case, the model really does just need to retain which word is in a particular point in the sentence and at the output representation corresponding to that point, conditioned on that, make a prediction of what that word was. But in order for BERT to be an effective language processor, the authors wanted it to also be able to, to be aware of the flow of how meaning works on a longer scale than just within a particular sentence. So in order to achieve this, they came up with an additional mechanism for training the weights in the BERT model, which is complimentary to the masked language model objective. So the way this works is the authors add an additional input token at the start and it's the output embedding corresponding to that input location that's going to be used to make the prediction on this objective. At the end, the model produces representations for corresponding to each of the input tokens, but it's only the initial representation corresponding to this additional token that was added to the inputs that needs to be considered in this objective. So like combining next sentence prediction and masked language modeling, slowly the weights of this large transformer, the BERT transformer, gradually start to acquire knowledge of how words interact in sentences typically maybe abstract knowledge of the typical ways in which meaning flows through sentences. And of course, the spaces in which they represent each of the individual words at various levels of the stack, things start to happen like words that have similar meanings start to come close together. And so, a lot of the general knowledge that we talked about being very necessary for forming a consistent and coherent representation of loads of different language sentences can start to be introduced into the weights of this model as it trains according to these unsupervised objectives. And in this case, the way that BERT is then evaluated is by taking its knowledge in all of those ways and using that as a start process to train on many specific language understanding tasks. So, in order to apply BERT to these models, the BERT weights, which are trained on all of the unsupervised objectives, are then taken and the data specific to each of these tasks is passed through the BERT model and then BERT is, the BERT weights are updated according to the signal from the supervised learning signal from these actual specific language understanding tasks. Typically this process of fine tuning the BERT representations for a specific task takes place separately and independently for each of those additional tasks. And it's also necessary in many cases when fine tuning in this way to add in a little bit of machinery onto the top of BERT because, you know, in the standard BERT architecture, it's just making predictions where it outputs a number of distributed representations at the top of this transformer model. But, of course, given a specific task, it may be necessary to come to some sort of prediction, depending on the output format of the task, it may be necessary to take only some of those representations and condition on them with additional weights in order to make that prediction. What I mean by that is, any model which is intending to be trained on a wide range of different tasks, using the BERT style approach, so transferring knowledge from an enormous running text corpora, via fine tuning to those specific tasks, has led to a really strong and significant performance on a large number of these tasks. And importantly, this doesn't just allow one model to solve lots of tasks better, in many cases, this is the way to achieve state-of-the-art performance on these additional tasks. So even a model which was just specialised to those additional supervised learning tasks would not perform better than a model that was initially pre-trained on BERT, in fact, you know, for a lot of these tasks, performance is substantially worse unless you apply BERT-style pre-training on enormous corpus before transferring to these additional tasks. Now a few years before BERT, a model called ELMO and a couple of other models, started to show that there was some promise in sharing more than just those word embedding weights, but actually sharing a large amount of functions which learned to combine weights when pretrained on some task agnostic objective and transferred to specific tasks. And then the BERT model really took that to the next level using the machinery of the transformer to exhibit really impressive transfer learning. So in the next section we'll look a bit forward to other sources of information which may plausibly be useful for different language understanding models because of course, BERT only has the means to acquire knowledge through text whereas if you think about the fruit fly example or time flying like an arrow, those sorts of examples tell us that there are many other sources of information that we may have used in order to gain the general conceptual or world knowledge required to actually make sense of language. And this works in part because of the critical importance of general knowledge in understanding language and we need ways in which models can acquire general principles of how language works and how word meanings fit together in order to make high quality predictions for a range of different language tasks. And in particular in a way that's not accessible to the BERT model, which is the ability to extract knowledge, general knowledge and conceptual knowledge from our surroundings, which is something as humans that we are doing all the time. So as well as the objective of masked language modeling and next sentence prediction that we saw with BERT, there's also exciting techniques in the field of computer vision, that often involve things like missing parts of an image and making predictions about whether or not that part of the image is the correct part or of which pixels would most appropriately fit in to that part of an image or maybe contrasting incorrect parts of images with correct parts of images and things like that. And, of course, in the world of learning when it comes to jointly learning language and behaviour, which involves often reinforcement learning on those sorts of tasks are techniques for having agents develop a more robust understanding of their surroundings and possibly import what's known as a model of their world. So in DeepMind we thought it was the right time given all of these improvements to start to study this question of knowledge acquisition through prediction in an actual agent that can interact in its surroundings. And the aim with this work was to study precisely whether or not an agent which moves around this world can apply various different algorithms in order to acquire as much knowledge as possible from its environment. And importantly, being able to answer these questions requires a particular type of knowledge, that's propositional knowledge, the knowledge, the ability to tell whether something's true or false in our environment and that's often contrasted especially by philosophers with procedural knowledge, which is just the sort of instinctive knowledge that maybe a reinforcement learning agent would naturally have when it learns to solve control problems in a very fast and precise way. So the agent just has to, the learning algorithm just has to find a way based on that experience to aggregate general knowledge into the agent such that when the question the QA decoder is queued with the state of the agent at the end of the episode and queued with the question, it's possible to combine those two pieces of knowledge and answer with 'dinosaur'. So to do this effectively, the agent needs a large amount of general knowledge about how things are arranged in the environment around it such that the QA decoder can take that knowledge and make predictions about the answers to questions. A little bit like the sorts of masked language model prediction that BERT's making, but where the agent has to given a certain time point in the episode, a predictive loss, a predictive engine an overshoot engine, takes the current memory state of the agent at that time point and rolls forward in time. And we tried two specific algorithms: so in one case in the SimCore algorithm, which was proposed last year, the loss that's used in this predictive mechanism is a generative model loss, which is modeling each of the individual pixels in the observations that the agent sees in the world in future timestamps and in the other predictive objective, we use contrastive predictive coding. This is basically asking the model to distinguish or maybe, presenting the model with two images at a given timestep and asking the model to say which of those two is actually the one that the agent encounters in the future as opposed to one which is selected randomly from some other episodes. Now we can evaluate these sorts of predictive mechanisms for aggregating knowledge in the agent precisely by their ability to create knowledge in the memory state of the agent such that at the final timestep of every episode, the question answering decoder can take that knowledge and answer the question. What we found surprisingly is that only one of these predictive algorithms actually led to the agent being able to effectively answer questions. And that was the SimCore, the model which uses a generative model to estimate the probability density of the pixels in future observations of the model conditioned on the memory state further back in time. So by doing that, you allow the agent to specialise in a particular type of question for every episode rather than requiring it to build up knowledge in a general way. But we exhibit, we observed these sorts of effects only in the agent which was endowed with the SimCore model of predicting the probabilities of pixels of future observations conditioned on the actions that it took at arbitrary overshoots into the future. So that's just a small insight into work that's going on in DeepMind where we're starting to consider how we can aggregate knowledge from the general environment as well as knowledge from large amounts of text into a single model which can start to combine this sort of conceptual understanding and general knowledge understanding and our understanding and a really strong understanding of language into a single agent which can come up with a coherent and strong ability to form the meaning of statements and sentences and also to take that knowledge to answer questions, to produce language and to enact policies, enabling it to do things things in complex environments. So our models are not currently able to do things like understanding the intentions of others or reflecting on how language is used to communicate and do things. So yeah, just as a final note, I think it's interesting that before deep learning really exhibited its success on language processing problems, a typical view of language understanding was what I call the pipeline view, which was that each independent, each part of processing language from the letters to the words, to the syntax and to the meaning and then eventually to some prediction could be thought of relatively independently as a separate process.
