Hello and welcome to the UCL and DeepMind lecture series. My name's Felix Hill and I'm going to be talking to you about deep learning and language understanding. So here's an overview of the structure of today's talk. It's going to be divided into four sections. And that model is the transformer which was released in 2018 and then in section three I'll go a bit deeper into a particular application of the transformer, that's the well known BERT model, and BERT in particular is an impressive demonstration of unsupervised learning and the ability of neural language models to transfer knowledge from one training environment to another. Also speech recognition and speech synthesis are really important applications that I won't have time to talk about. I mean, it's not because I think they're more important or more valid than any other areas. So you may have heard of models like GPT-2 or BERT, or WaveNet, which was developed in DeepMind. So on the left we have systems which are almost now entirely based on neural networks from machine translation systems to speech synthesis systems and speech recognition and then on the right here, it's important to note that there are still many applications which do language processing but don't use deep learning or neural networks for all of their computation or even at all. And it's not just in practical applications. In the slightly more focused world of research, we see a similar trend. And you can see that back in 2010 there was close to or effectively zero papers with those words in the title. But by the time we got to 2016 this number has scaled up rapidly. You can see the rate of improvement from under 60% performance to over approximately 85% performance. But of course, in order to understand that, we really need to think a bit about language. But of course, if those who think a little bit more about language specifically, have many reasons to believe that individual words that we might be passing to these models don't seem to behave like discrete symbols exactly. we could see the clock face from below', 'it could be time to face his demons' or 'there are a few new faces in the office today'. Now the face, which is the most important side of somebody's head of sight for the eyes and the nose. And to give just a small example of how our language processing really depends on context, consider this example. So he noticed that if we had some handwriting like this, 'Jack and Jill went up the hill', we can read it very quickly. But if you look at the areas highlighted in red here, you'll see that the, there's actually a character which is identical in both cases. And in particular, what we probably do according to this example at least, is think about the whole sentence and think what's the most likely interpretation of the whole sentence and that in itself informs the individual interpretation of the particular characters where ambiguity might be. Another classic example of this phenomenon can be simply gained by reading the following symbol on your screen, the following image by reading it across or down. So we've seen then that words are not necessarily best modelled as discrete symbols. Classic example is sentences a bit like this: 'The man who ate the pepper sneezed'. However, there are of course other factors at play. Whereas, our knowledge of peppers will tell us that they don't typically sneeze and therefore we don't think that the pepper sneeze is a very likely state of affairs and we look for other ways to make sense of the sentence and the correct way of making sense of that sentence, of the sentence in fact is more salient to us as we process it. So that's just a thought to bear in mind when we're thinking about optimal processes of language in deep learning models. And it shouldn't operate arbitrarily on any different set of inputs. It could even be black in that particular image. Things get even more wacky in certain cases. But when we think about pet fish, this sort of magic seems to happen where our typical pet fish has lots of bright colours. It could be orange, green or purple or yellow. So something seems to have happened in our mind to allow these strong features to come into the representation of pet fish which didn't play a strong role in our representations of either pet or fish. This doesn't always happen when we combine words, but it does sometimes. Another example would be this representation of plant, which might be typically looks something like that and our representation of carnivore, which might be, look a bit like that, but our representation of carnivorous plant has this additional feature about eating insects. So just to summarise, we've seen in this section that words have many related senses and that they're not necessarily characterised as sort of perfectly idealised discrete symbols. So, in the first part we talked about particular aspects of language and particular aspects of neural computation that have that seem to fit together in a particularly appropriate way, such that, they define certain ways in which a computational model might need to behave in order to capture the ways that meaning works in language. So just here's credit to the authors of the transformer from Google Brain and collaborators. And what do I mean by a distributed representation of words? Well, the first thing that we do when we construct a neural language model is we have to determine what is the vocabulary on which the model is going to operate. Now, if you think about a large page of text, those units could be individual characters. In an extreme case, they could be individual pixels if we consider that the, the text an actual image. But of course, as we've talked about in the last section, a model which just takes symbols and treats them as symbols might not be optimal for capturing all of the aspects of meaning that we see in natural language. So instead of doing that, the developers of neural language models have come up with a procedure which allows the model to be more flexible than which would be represents, in the ways of which it represents words. And that process is something like the following. So to get such a list, we might scan hundreds of thousands of pages of texts and count all the works that we find there, and then we can take some subset of the words which appear the most frequently or alternatively if we have lots of memory and a really big model, we can take all of the words and allow all of those to be in the vocabulary of the model. Now that dimension we can think of as the word representation dimension or the word embedding dimension and when the model sees a given word, we turn on the unit corresponding to that word and we leave all of the other units at zero. So we put an activation of one on unit corresponding to the word leave all of the other weights as zero and we've marked those weights in this diagram here with yellow and light blue shows the space occupied by the whole layer of input weights for the model. Now this idea of representing words or letters or whatever we take as the input units to a model in some sort of high dimensional floating value, real valued vector space is actually quite an old idea. And of course, perhaps the most famous example of this demonstration came from a very famous paper in which Jeff Elman introduced the recurrent neural network to the wider community. So that's the solid foundation on which the transformer builds. So the first matrix is we could call the query weight matrix WQ. And each of these matrices have independent weights in the transformer and we can, their dimensions are such that they can naturally multiply, in this case, I've written it as post multiplication of the distributed word vector that I talked about in the first section. So in particular with the query vector, we produce an operation where for every word we take the query vector corresponding to that word and we compute the inner product, the dot product of that word, with the value, with the, with the key vector corresponding to each of the other words. So this then gives us a set of weights correspond-, it's a probability distribution, which gives us a set of weights between zero and one, so for a given word 'beetle', we get a set of weights, one for each of the words in the input telling us to what extent is there a strong interaction between the word 'beetle' and that other word. So that's the sort of interaction that we want to naturally capture here. So in this case, for example, when representing the word 'beetle', we would notice a strong connection with the word 'drove' and that that would give us a strong weight in our attention distribution. So what we end up with then for each word like 'beetle' is that we take a small amount of the value of each of the other words plus some of the value of the word 'beetle' through to the next layer of the transformer. So notice that having performed this transformer layer, we haven't reduced the number of embeddings in the model in any way, we still have a representation corresponding to the word 'beetle' that we started with, but that representation has been updated or modulated, conditioned exactly on information about how well it corresponds or how well it should interact with all of the other words in the input. But it's not just this self-attention layer that gives the transformer it's expressability and power is actually an operation known as a multi-head self-attention, which basically takes the operation I just talked about and reapplies it four different times in parallel. Now of course that is a lot of computation and it might require a lot of memory if we end up with very large representations in our model. In this case, we might find it to be just 25 units. But it does give the model many independent ways with which it can represent interactions between the words and the inputs. It's not necessarily something we would associate with pets and it's not something we would necessarily associate with fish, ordinarily. And where does that additional notion of colours come from? So let's say that the model didn't have skip connections and fed things through to a certain level in the hierarchy. At that point after computing many different interactions, the model might form a consistent sense of the fact that a meaning needs to be understood in a particular way, but of course those top down influences tell us that that expectation of what the meaning might be should actually feed back and allow us to remodulate how we understand the input. There's one more detail I'll finish off with and our characterisation of the transformer. It was a series of matrix multiplications which were applied identically to each of the words. And then on top of that, a series of inner products, which are symmetric operations, which don't favour the ordering in which we apply them with respect to the words. But then it, it's harder for that model to remember to pay attention to things a long time in the past even if those things actually end up having a really important influence on what I'm currently looking at now. With a transformer, things are totally different. With a transformer that path that the gradient has to go through is much shorter because there's no prior favouring of things which are close together instead of the gradient path that the model needs to go through to connect any two words in the input is equivalent and in fact it indeed is shorter on average than it is in recurrent neural networks. So that gives a small amount of intuition about another reason why the transformer might be so effective at processing. So interactions over words that are next to each other are not particularly favoured over minor interactions. So hopefully this section has given you some intuition about how a transformer works, but also some intuition about maybe why it works, why it is that the various components in the transformer improve on a model's ability to process language because of the way that we think meaning works in a very sort of intricate and interactive way when we understand linguistic input. And those tasks might involve reading a sentence and making a prediction or classifying how two sentences relate to each other or even classifying or making predictions about longer texts such as documents. But before I do that, I also just want to go back to our points about the nature of language and discuss one more issue which I think is quite motivating when we think about how transformers are applied in the model that I'm going to talk about in this section. So let's consider this sentence: 'time flies like an arrow'. Another important piece of experience that we have is our experience of bunch of phrases or sentences which are quite similar to the phrase 'time flies like an arrow'. So those could be things like 'John works like a Trojan' or 'the trains run like clockwork'. So it feels to me like those two pieces of experience are very important in our ability to read a sentence, like 'time flies like an arrow' and immediately understand it. In the case of 'fruit flies like a banana' of course we come to quite a different understanding, right? And a lot of the places where such a model needs to get its understanding of the world and its understanding of language and those considerations lead us to add a fifth point to these many characteristics of language, which is that when we actually form an understanding, you know, it really does seem to be a process of balancing our existing knowledge and that could be knowledge of language and also knowledge of the world with the input with the particular facts of the thing that's currently coming into the model. I'm going to describe in the section which is called BERT. And BERT stands for Bi-directional Encoder Representations with Transformers. So the important thing to remember when considering how BERT works is that a transformer as described in the previous section really is just a mapping from a set of distributed word representations to another set of distributed word representations. And corresponding to each of the words in the input, the exact same set of words that were the input. So the job of this transformer in theory is just to represent a sentence, for example, and then output a sentence at the very top of its network. But rather than, rather than having the model output the exact same sentence, instead in the input to the model, one of the words is masked out. And when training the model, the authors of BERT do that with 15% of words at random. So that's masked language modal pretraining. It would never have to make any sort of unexpected judgment about what word could be missing. It would instead just be able to copy knowledge straight through. And that wouldn't lead to any interesting formation of any interesting representations. So this is only done occasionally, but it does make the model perform better on the test set because the model does not, kind of, find itself completely out of its training experience when it encountered sentences for which no words are masked out. So that's the masked language modeling objective. So in order to achieve this, they came up with an additional mechanism for training the weights in the BERT model, which is complimentary to the masked language model objective. So this objective can be trained at the same time as the masked language modeling objective. And as in that case, it doesn't require any data that's been labeled by experts or found to have a right answer in some way. So in this case it is two consecutive sentences: 'Sid went outside' and 'it began to rain'. So cases where one sentence didn't follow the other sentence. So that might look something like 'Sid went outside', 'unfortunately it wasn't'. So like combining next sentence prediction and masked language modeling, slowly the weights of this large transformer, the BERT transformer, gradually start to acquire knowledge of how words interact in sentences typically maybe abstract knowledge of the typical ways in which meaning flows through sentences. The model might require to separate them out into the different parallel heads if words have various different senses. And of course, because neither of those training objectives required any sort of particular labels you can, BERT is trainable on all of the texts that exist in digital form in English around the world. And these tasks typically have, they do use labeled data and they typically have a lot less data. Typically this process of fine tuning the BERT representations for a specific task takes place separately and independently for each of those additional tasks. So this is a really sort of compelling demonstration of transfer learning. But they didn't have a mechanism to encode the ways in which those words combined. And when we've added this fifth one: understanding is balancing input with knowledge that we've had already or our general knowledge of the world. So in the last section we saw how the BERT model is a really exciting example of transferring knowledge from an enormous amount of text, to apply that knowledge to very specific language tasks that maybe have a small amount of data from which to learn. So those sorts of objectives are also leading to really good ability to transfer from large banks of images to specific image classification tasks. Now, in our case, we unfortunately can't set an enormous neural network free in the world in which we live and just see if it learns. But the next best thing is to create a simulated world. And we did that in the unity game engine. And even comparison questions like things like 'is the number of rubber ducks bigger than the number of toy sheep?' So that essentially creates a video of experience and then we set our learning model the challenge of taking in that experience and aggregating knowledge as much as possible in the memory state of the agent as it lives that experience. And then the way that we measure the quality of that knowledge is by bolting on a QA decoder onto the agent. Now the agent would explore and the agent's learning algorithm can take in all of the things it sees as it moves around the room. But the agent itself can't see the question. So the weights and the agent and the objectives that the agents applying must be general. They can't be specifically tailored to getting the knowledge to answer the question. And then to this we apply various different baseline. Now, another approach is to endow the agent with predictive learning objectives. And we tried two specific algorithms: so in one case in the SimCore algorithm, which was proposed last year, the loss that's used in this predictive mechanism is a generative model loss, which is modeling each of the individual pixels in the observations that the agent sees in the world in future timestamps and in the other predictive objective, we use contrastive predictive coding. Now we can evaluate these sorts of predictive mechanisms for aggregating knowledge in the agent precisely by their ability to create knowledge in the memory state of the agent such that at the final timestep of every episode, the question answering decoder can take that knowledge and answer the question. And that was the SimCore, the model which uses a generative model to estimate the probability density of the pixels in future observations of the model conditioned on the memory state further back in time. So the contrastive predictive algorithm was much less effective at giving the agent the general knowledge required to be able to answer these questions. The green line at the top of the plot here shows the performance of the agent if you backpropagate from the question answers back into the actual agent memory. So by doing that, you allow the agent to specialise in a particular type of question for every episode rather than requiring it to build up knowledge in a general way. And you can see that as the episode continues, the agent's prediction gets more and more confident that the answer is red. Such that on the final timestep red is by far the most probable answer. If we consider the other video, let me just activate the other video. You can see that a similar thing happens with a different type of question. So here the question is 'what is the aquamarine object?' And the answer is it's a grinder, it's a salt and pepper grinder. So we've talked about various aspects of language which make neural networks and deep networks particularly appropriate models for capturing the way that meaning works. It can require, it can depend very much on what we're currently seeing or doing. And distributed representations are particularly effective for language processing. And that's in particular around a lot of the social aspects of language understanding. But now that we've reflected on how language works and in particular taking in all of the evidence from the effectiveness of different neural language models on language processing tasks, I think maybe this is a more effective or more realistic schematic of how language processing should be thought of. At the end, I've popped a few there for recent work at DeepMind but again, there's no, not time to list a huge amount of very related work.